---
title: "[KAIST AI 교육 :: 데이터 분석 및 예측] 2"
date: 2020-08-18 23:15:30 -0400
categories: DeepLearning DataAnalyze
---
# 선형회귀와 경사하강법

<hr/>
<div style = "font-size :0.7em">

<p>
오늘 다룰 것은 머신러닝의 가장 기초적인 추론법인 선형회귀.<br/><br/>

공대생이라면 절대 모를 리 없는 '선형'이지만 요즘 머신러닝이 인기가 많기에 비전공자들도
머신러닝을 하기에 조금 설명하자면..<br/><br/>

선형(linear). 이름에서 유추 할 수 있듯 하나의 직선으로 표현되는 것이고 그러한 성질을 띄는 것을 선형성이라 한다.<br/><br/>

엄밀히 이야기 하자면 이론적인 선형성은 vector에서 이야기 되며, vector에 특징 답게 기울기만 같다면 같은 걸로 퉁친다.<br/><br/>

걍 y절편, 출발점, 길이 이딴거 때려친다는 소리다.<br/><br/>

그냥 간략하게 우리가 중학교에서 배우는 1차함수인 y = ax꼴을 생각하면 된다.<br/><br/><br/>

<i>선형성은 중첩의 원리를 보장 (ax1+bx2 = a(x1+x2).가 통한다.)하며 중첩의 원리가 통하는 식들은 직관적이고 예측 가능하다.<br/>

이 때문에 논문들에서 내가 만든 모델이 선형성을 띈다는 것을 증명하는데 하루종일 써내리는 이유기도 하다.</i><br/><br/><br/>


</p>
<hr/>
<p>
이제 선형회귀에 대해 다시 설명하자면 선형식인 H(x)를 찾아내는 것이다.
그리고 H(x)가 x하나에 대해서 변한다면 단순 회귀 분석, 둘 이상이라면 다중 회귀 분석이라 한다.<br/>

단순 : H(x) = Wx1+b<br/>
다중 : H(x) = W1x1 + W2x2 +....Wnxn+b<br/>

<br/><br/>

</p>
<div/>